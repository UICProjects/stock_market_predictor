{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.models import Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss, confusion_matrix, accuracy_score,precision_score,recall_score,f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kapsu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kapsu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kapsu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data = pd.read_csv(\"Data/news_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>datetime</th>\n",
       "      <th>headline</th>\n",
       "      <th>id</th>\n",
       "      <th>image</th>\n",
       "      <th>related</th>\n",
       "      <th>source</th>\n",
       "      <th>summary</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>company</td>\n",
       "      <td>1697909459</td>\n",
       "      <td>Humanoid robots face a major test with Amazon'...</td>\n",
       "      <td>123320327</td>\n",
       "      <td>https://techcrunch.com/wp-content/uploads/2023...</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>Yahoo</td>\n",
       "      <td>Announced amid a deluge of news at this week’s...</td>\n",
       "      <td>https://finnhub.io/api/news?id=8cb32870bb15be8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>company</td>\n",
       "      <td>1697896800</td>\n",
       "      <td>3 Stocks That Turned $1,000 Into $1.1 Million ...</td>\n",
       "      <td>123315781</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MNST</td>\n",
       "      <td>Yahoo</td>\n",
       "      <td>Consumer products have proven themselves to be...</td>\n",
       "      <td>https://finnhub.io/api/news?id=d75d610543b91f1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>company</td>\n",
       "      <td>1697896800</td>\n",
       "      <td>The U.S. Cities With the Most Cutting-Edge Tec...</td>\n",
       "      <td>123315249</td>\n",
       "      <td>https://s.yimg.com/ny/api/res/1.2/kLEZByvMTM7D...</td>\n",
       "      <td>WMT</td>\n",
       "      <td>Yahoo</td>\n",
       "      <td>The Seattle area has the highest proportion of...</td>\n",
       "      <td>https://finnhub.io/api/news?id=faca644c2f7e4e6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>company</td>\n",
       "      <td>1697895295</td>\n",
       "      <td>Dow Jones Futures: Market Point Break? Microso...</td>\n",
       "      <td>123315250</td>\n",
       "      <td>https://media.zenfs.com/en/ibd.com/ec8365e261a...</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>Yahoo</td>\n",
       "      <td>Microsoft, Meta and Google lead a massive wave...</td>\n",
       "      <td>https://finnhub.io/api/news?id=6c9b9cc8fb7f9ec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>company</td>\n",
       "      <td>1697890800</td>\n",
       "      <td>Rivian (NASDAQ:RIVN): Wall Street Loves This E...</td>\n",
       "      <td>123319798</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>TipRanks</td>\n",
       "      <td>Looking for stock market analysis and research...</td>\n",
       "      <td>https://finnhub.io/api/news?id=fe0b89552f14e89...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  category    datetime                                           headline  \\\n",
       "0  company  1697909459  Humanoid robots face a major test with Amazon'...   \n",
       "1  company  1697896800  3 Stocks That Turned $1,000 Into $1.1 Million ...   \n",
       "2  company  1697896800  The U.S. Cities With the Most Cutting-Edge Tec...   \n",
       "3  company  1697895295  Dow Jones Futures: Market Point Break? Microso...   \n",
       "4  company  1697890800  Rivian (NASDAQ:RIVN): Wall Street Loves This E...   \n",
       "\n",
       "          id                                              image related  \\\n",
       "0  123320327  https://techcrunch.com/wp-content/uploads/2023...    AMZN   \n",
       "1  123315781                                                NaN    MNST   \n",
       "2  123315249  https://s.yimg.com/ny/api/res/1.2/kLEZByvMTM7D...     WMT   \n",
       "3  123315250  https://media.zenfs.com/en/ibd.com/ec8365e261a...    MSFT   \n",
       "4  123319798                                                NaN    TSLA   \n",
       "\n",
       "     source                                            summary  \\\n",
       "0     Yahoo  Announced amid a deluge of news at this week’s...   \n",
       "1     Yahoo  Consumer products have proven themselves to be...   \n",
       "2     Yahoo  The Seattle area has the highest proportion of...   \n",
       "3     Yahoo  Microsoft, Meta and Google lead a massive wave...   \n",
       "4  TipRanks  Looking for stock market analysis and research...   \n",
       "\n",
       "                                                 url  \n",
       "0  https://finnhub.io/api/news?id=8cb32870bb15be8...  \n",
       "1  https://finnhub.io/api/news?id=d75d610543b91f1...  \n",
       "2  https://finnhub.io/api/news?id=faca644c2f7e4e6...  \n",
       "3  https://finnhub.io/api/news?id=6c9b9cc8fb7f9ec...  \n",
       "4  https://finnhub.io/api/news?id=fe0b89552f14e89...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data.drop(columns=['category','image','url','id','source'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['datetime', 'headline', 'related', 'summary'], dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(757, 4)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>headline</th>\n",
       "      <th>related</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1697909459</td>\n",
       "      <td>Humanoid robots face a major test with Amazon'...</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>Announced amid a deluge of news at this week’s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1697896800</td>\n",
       "      <td>3 Stocks That Turned $1,000 Into $1.1 Million ...</td>\n",
       "      <td>MNST</td>\n",
       "      <td>Consumer products have proven themselves to be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1697896800</td>\n",
       "      <td>The U.S. Cities With the Most Cutting-Edge Tec...</td>\n",
       "      <td>WMT</td>\n",
       "      <td>The Seattle area has the highest proportion of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1697895295</td>\n",
       "      <td>Dow Jones Futures: Market Point Break? Microso...</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>Microsoft, Meta and Google lead a massive wave...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1697890800</td>\n",
       "      <td>Rivian (NASDAQ:RIVN): Wall Street Loves This E...</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>Looking for stock market analysis and research...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     datetime                                           headline related  \\\n",
       "0  1697909459  Humanoid robots face a major test with Amazon'...    AMZN   \n",
       "1  1697896800  3 Stocks That Turned $1,000 Into $1.1 Million ...    MNST   \n",
       "2  1697896800  The U.S. Cities With the Most Cutting-Edge Tec...     WMT   \n",
       "3  1697895295  Dow Jones Futures: Market Point Break? Microso...    MSFT   \n",
       "4  1697890800  Rivian (NASDAQ:RIVN): Wall Street Loves This E...    TSLA   \n",
       "\n",
       "                                             summary  \n",
       "0  Announced amid a deluge of news at this week’s...  \n",
       "1  Consumer products have proven themselves to be...  \n",
       "2  The Seattle area has the highest proportion of...  \n",
       "3  Microsoft, Meta and Google lead a massive wave...  \n",
       "4  Looking for stock market analysis and research...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int64\n"
     ]
    }
   ],
   "source": [
    "print(news_data['datetime'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data['y'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fetch_stock_data_for_date import StockData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = StockData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_data(x):\n",
    "    try:\n",
    "        return sd.get_delta(x['datetime'], x['related'])\n",
    "    except ValueError as ve:\n",
    "        print(ve)\n",
    "        errors.append(x)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data['y'] = news_data.apply(lambda x : get_stock_data(x),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    757.000000\n",
       "mean      -1.324373\n",
       "std        5.709280\n",
       "min     -136.899902\n",
       "25%       -2.040001\n",
       "50%       -0.550003\n",
       "75%        0.190001\n",
       "max        7.330002\n",
       "Name: y, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data['y'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>headline</th>\n",
       "      <th>related</th>\n",
       "      <th>summary</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1697909459</td>\n",
       "      <td>Humanoid robots face a major test with Amazon'...</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>Announced amid a deluge of news at this week’s...</td>\n",
       "      <td>-2.880005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1697896800</td>\n",
       "      <td>3 Stocks That Turned $1,000 Into $1.1 Million ...</td>\n",
       "      <td>MNST</td>\n",
       "      <td>Consumer products have proven themselves to be...</td>\n",
       "      <td>-0.420002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1697896800</td>\n",
       "      <td>The U.S. Cities With the Most Cutting-Edge Tec...</td>\n",
       "      <td>WMT</td>\n",
       "      <td>The Seattle area has the highest proportion of...</td>\n",
       "      <td>-2.070007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1697895295</td>\n",
       "      <td>Dow Jones Futures: Market Point Break? Microso...</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>Microsoft, Meta and Google lead a massive wave...</td>\n",
       "      <td>-5.049988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1697890800</td>\n",
       "      <td>Rivian (NASDAQ:RIVN): Wall Street Loves This E...</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>Looking for stock market analysis and research...</td>\n",
       "      <td>-5.019989</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     datetime                                           headline related  \\\n",
       "0  1697909459  Humanoid robots face a major test with Amazon'...    AMZN   \n",
       "1  1697896800  3 Stocks That Turned $1,000 Into $1.1 Million ...    MNST   \n",
       "2  1697896800  The U.S. Cities With the Most Cutting-Edge Tec...     WMT   \n",
       "3  1697895295  Dow Jones Futures: Market Point Break? Microso...    MSFT   \n",
       "4  1697890800  Rivian (NASDAQ:RIVN): Wall Street Loves This E...    TSLA   \n",
       "\n",
       "                                             summary         y  \n",
       "0  Announced amid a deluge of news at this week’s... -2.880005  \n",
       "1  Consumer products have proven themselves to be... -0.420002  \n",
       "2  The Seattle area has the highest proportion of... -2.070007  \n",
       "3  Microsoft, Meta and Google lead a massive wave... -5.049988  \n",
       "4  Looking for stock market analysis and research... -5.019989  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words_and_puncts(words):\n",
    "    f1 = [w.lower() for w in words if w.lower() not in stop_words]\n",
    "    f2 = [w for w in f1 if w not in string.punctuation]\n",
    "    return f2\n",
    "\n",
    "def process_text(text):\n",
    "    words = word_tokenize(text)\n",
    "    clean_words = remove_stop_words_and_puncts(words)\n",
    "    clean_words = [w for w in clean_words if w.isalpha()]  # Remove non-alpha text\n",
    "    lemmatized_words = [\n",
    "        lemmatizer.lemmatize(w) for w in clean_words\n",
    "    ]  # Lemmatization\n",
    "    return lemmatized_words\n",
    "\n",
    "def process_text_as_string(text):\n",
    "    lemma_words = process_text(text)\n",
    "    \n",
    "    if len(lemma_words) >= 3:\n",
    "        return \" \".join(lemma_words)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "modded = news_data['summary'].apply(lambda x : process_text_as_string(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data['pt'] = modded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data = news_data[news_data['pt'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(use_idf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the processed text data\n",
    "vectorized_data = vectorizer.fit_transform(news_data['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert the sparse matrix to an array for inspection (if needed)\n",
    "# vectorized_data_array = vectorized_data.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['070', '0981', '10', ..., 'zero', 'zhou', 'zuckerberg'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: Get the feature names\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<724x3846 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 19563 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: Print the vectorized data\n",
    "vectorized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_data.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(724, 3846)\n"
     ]
    }
   ],
   "source": [
    "# Preparing test and train data sets\n",
    "print(vectorized_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = news_data['y'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels > 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(vectorized_data, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(579, 3846)\n",
      "(579,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(145, 3846)\n",
      "(145,)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize classifiers\n",
    "classifiers = {\n",
    "    'SVM_Linear': SVC(kernel='linear'),\n",
    "    'SVM_poly': SVC(kernel='poly', degree=3),\n",
    "    'SVM_rbf': SVC(kernel='rbf', C=1.0, gamma='scale'),\n",
    "    'SVM_sigmoid': SVC(kernel='sigmoid'),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=42)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM_Linear Accuracy: 71.72%\n",
      "SVM_poly Accuracy: 70.34%\n",
      "SVM_rbf Accuracy: 70.34%\n",
      "SVM_sigmoid Accuracy: 71.72%\n",
      "Random Forest Accuracy: 69.66%\n",
      "Naive Bayes Accuracy: 71.03%\n",
      "K-Nearest Neighbors Accuracy: 73.10%\n",
      "Logistic Regression Accuracy: 71.03%\n",
      "Gradient Boosting Accuracy: 65.52%\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate each classifier\n",
    "output_data = []\n",
    "for clf_name, clf in classifiers.items():\n",
    "    # Train the model on the training data\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    predictions = clf.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, predictions)*100\n",
    "    # precision = precision_score(y_test,predictions)*100\n",
    "    # recall = recall_score(y_test,predictions)*100\n",
    "    # f1 = f1_score(y_test,predictions) * 100\n",
    "    print(f\"{clf_name} Accuracy: {accuracy:.2f}%\")\n",
    "    # print(f\"{clf_name} Precision: {precision:.2f}%\")\n",
    "    # print(f\"{clf_name} Recall: {recall:.2f}%\")\n",
    "    # print(f\"{clf_name} F1 score: {f1:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
