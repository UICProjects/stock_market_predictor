# -*- coding: utf-8 -*-
"""Project -  ML - Classifier  Models

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1h7S7LABLX3MPLdM0JC--nkAXwt9pBQkz
"""

# Reading News data

import pandas as pd

# Replace 'file_path' with the path to your CSV file
file_path = 'news_data.csv'

# Load the CSV file into a DataFrame
data_news = pd.read_csv(file_path).values

# Select specific columns by their column indices (0-based indexing)
selected_columns = [1, 2, 5]  # Example: Selecting columns 1, 3, and 5

# Extract the selected columns and convert them into a NumPy array
data_news_subset = data_news[:, selected_columns]

data_news_subset[:,0] = pd.to_datetime(data_news_subset[:,0],unit = 's').date

print(data_news_subset[0])

# Reading historical stock prices

# Replace 'file_path' with the path to your CSV file
file_path = 'historical_stock_prices.csv'

# Load the CSV file into a DataFrame
data_stocks = pd.read_csv(file_path).values

# Select specific columns by their column indices (0-based indexing)
selected_columns = [0, 1, 4, 7]  # Example: Selecting columns 1, 3, and 5

# Extract the selected columns and convert them into a NumPy array
data_stocks_subset = data_stocks[:, selected_columns]

data_stocks_subset[:,0] = pd.to_datetime(data_stocks_subset[:,0]).date

print(data_stocks_subset[0])

# merge two data sets using inner join - ticker and date

data_news_df = pd.DataFrame(data_news_subset)
data_stocks_df = pd.DataFrame(data_stocks_subset)

merged_data = pd.merge(data_news_df, data_stocks_df, left_on=[0,2], right_on=[0,3], how='inner')


print(merged_data)

# data set with X and label
import numpy as np

dataset = np.column_stack((merged_data.values[:,2], np.where(merged_data.values[:,5] > merged_data.values[:,4], 1, 0)))

labels = np.where(merged_data.values[:,5] > merged_data.values[:,4], 1, 0)

print(dataset[:3,:])

# preprocessing

from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from gensim.models import Doc2Vec
import nltk
from nltk.tokenize import word_tokenize, RegexpTokenizer
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn import naive_bayes
import string
import numpy as np

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import log_loss, confusion_matrix, accuracy_score,precision_score,recall_score,f1_score

nltk.download('stopwords')
stop_words = set(stopwords.words('english'))
nltk.download('punkt')
nltk.download('wordnet')

# Remove Blank rows in Data
# Change all the text to lower case
# Word Tokenization
# Remove Stop words
# Word Lemmatization

keys = ['text']
dataset = [{key: value for key, value in zip(keys, row)} for row in dataset]

if isinstance(dataset, list) and all(isinstance(row, dict) for row in dataset):
    # Remove blank rows
    dataset = [row for row in dataset if row['text'].strip()]

    stop_words = set(stopwords.words('english'))
    lemmatizer = WordNetLemmatizer()

    def remove_stop_words_and_puncts(words):
        f1 = [w.lower() for w in words if w.lower() not in stop_words]
        f2 = [w for w in f1 if w not in string.punctuation]
        return f2

    def process_text(text):
        words = word_tokenize(text['text'])
        clean_words = remove_stop_words_and_puncts(words)
        clean_words = [w for w in clean_words if w.isalpha()]  # Remove non-alpha text
        lemmatized_words = [lemmatizer.lemmatize(w) for w in clean_words]  # Lemmatization
        return lemmatized_words

    docs = [process_text(doc) for doc in dataset]
    print(docs)
else:
    print("The dataset is not in the expected format (list of dictionaries).")

#Vectorization

from sklearn.feature_extraction.text import TfidfVectorizer

# Joining the processed words into strings for vectorization
docs_processed = [' '.join(doc) for doc in docs]

# Initializing TfidfVectorizer
vectorizer = TfidfVectorizer()

# Fit and transform the processed text data
vectorized_data = vectorizer.fit_transform(docs_processed)

# # Convert the sparse matrix to an array for inspection (if needed)
# vectorized_data_array = vectorized_data.toarray()

# Example: Get the feature names
feature_names = vectorizer.get_feature_names_out()

# Example: Print the vectorized data
print("Vectorized Data:\n", vectorized_data)

# Preparing test and train data sets

print(vectorized_data.shape)

# Splitting the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(vectorized_data, labels, test_size=0.2, random_state=42)

print(X_train.shape)
print(y_train.shape)

print(X_test.shape)
print(y_test.shape)

from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score
from sklearn.feature_extraction.text import TfidfVectorizer


# Initialize classifiers
classifiers = {
    'SVM_Linear': SVC(kernel='linear'),
    'SVM_poly': SVC(kernel='poly', degree=3),
    'SVM_rbf': SVC(kernel='rbf', C=1.0, gamma='scale'),
    'SVM_sigmoid': SVC(kernel='sigmoid'),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'Naive Bayes': MultinomialNB(),
    'K-Nearest Neighbors': KNeighborsClassifier(),
    'Logistic Regression': LogisticRegression(),
    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=42)
}

# Train and evaluate each classifier
output_data = []
for clf_name, clf in classifiers.items():
    # Train the model on the training data
    clf.fit(X_train, y_train)

    # Make predictions on the test data
    predictions = clf.predict(X_test)

    # Calculate accuracy
    accuracy = accuracy_score(y_test, predictions)*100
    print(f"{clf_name} Accuracy: {accuracy:.2f}%")